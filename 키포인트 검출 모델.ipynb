{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8272661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08259945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Tuple, Sequence, Callable, Dict\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "# from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Tuple \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42490526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 수 있도록 데이터 전처리 클래스\n",
    "\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: os.PathLike,\n",
    "        label_df: pd.DataFrame,\n",
    "        transforms: Sequence[Callable]=None\n",
    "    ) -> None:\n",
    "        self.image_dir = image_dir\n",
    "        self.df = label_df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Dict]:\n",
    "        image_id = self.df.iloc[index, 1]\n",
    "        labels = np.array([1])\n",
    "\n",
    "        keypoints = self.df.iloc[index, 2:].values.reshape(-1, 2).astype(np.int64)\n",
    "    \n",
    "        x1, y1 = min(keypoints[:, 0]), min(keypoints[:, 1])\n",
    "        x2, y2 = max(keypoints[:, 0]), max(keypoints[:, 1])\n",
    "        \n",
    "\n",
    "        \n",
    "        boxes = np.array([[x1, y1, x2, y2]], dtype=np.int64)\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        img_array = np.fromfile(image_path, np.uint8)\n",
    "        image = cv2.imdecode(img_array, cv2.COLOR_BGR2RGB)\n",
    "        targets ={\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels,\n",
    "            'keypoints': keypoints\n",
    "        }\n",
    "#         print('bboxes : ' , boxes)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            targets = self.transforms(**targets)\n",
    "\n",
    "        image = targets['image']\n",
    "        image = image / 255.0\n",
    "   \n",
    "\n",
    "        targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((15, 1))], axis=1)[np.newaxis], dtype=torch.float32\n",
    "            )\n",
    "        }\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e5d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 저장한 파일을 불러와서 학습용/ 검즘용 데이터로 나눠서 전처리 함수에 전달\n",
    "\n",
    "def collate_fn(batch: torch.Tensor)->Tuple:\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Data Transform & Train-Test-Split\n",
    "def load_data(image_dir, train_key, valid_key):\n",
    "    transforms = A.Compose([\n",
    "        # A.Resize(500, 500, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ],  bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "        keypoint_params=A.KeypointParams(format='xy')\n",
    "    )\n",
    "\n",
    "    trainset = KeypointDataset(image_dir, train_key, transforms)\n",
    "    validset = KeypointDataset(image_dir, valid_key, transforms)\n",
    "    train_loader = DataLoader(trainset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(validset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a58ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> nn.Module:\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained=True, trainable_layers = 2) # resnet101, resnet152 \n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    keypoint_roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=14,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = KeypointRCNN(\n",
    "        backbone, \n",
    "        num_classes=2,\n",
    "        num_keypoints=15,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        keypoint_roi_pool=keypoint_roi_pooler\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba55cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device = 'cuda'):\n",
    "    model.train()                                        \n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses = model(images, targets)\n",
    "        loss = losses['loss_keypoint']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 200 == 0:\n",
    "            print(f'| epoch: {epoch} | batch: {batch_idx+1}/{len(train_loader)} | batch loss: {loss.item()}')\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, device = 'cuda'):\n",
    "    model.train()      \n",
    "    test_loss = 0      # test_loss 초기화\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for images, targets in test_loader:\n",
    "            # data, target 값 DEVICE에 할당\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  \n",
    "\n",
    "            losses = model(images, targets)                       # validation loss\n",
    "            test_loss += float(losses['loss_keypoint'])           # sum of all loss \n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)                         # 평균 loss\n",
    "    return test_loss\n",
    "\n",
    "def train_model(train_loader, val_loader, model_path=None, num_epochs=30, device='cuda'):\n",
    "    if not os.path.exists(model_path):\n",
    "        model = get_model()\n",
    "    else:\n",
    "        model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = 999999  # initialize best loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer, epoch, device)\n",
    "        train_loss = train(model, train_loader, optimizer, epoch, device)\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        print('Train Keypoint Loss (avg): {:.4f}'.format(train_loss))\n",
    "\n",
    "        if val_loss <= best_loss:   # update best loss\n",
    "            best_loss = val_loss\n",
    "            torch.save(model, './models/RCNN_ep'+str(epoch)+'_'+str(best_loss)+'.pt')\n",
    "            print('Best Model Saved, Loss: ', val_loss)\n",
    "        \n",
    "        time_elapsed = time.time()-since\n",
    "        print()\n",
    "        print('---------------------- epoch {} ------------------------'.format(epoch))\n",
    "        print('Train Keypoint Loss: {:.4f}, Val Keypoint Loss: {:.4f}'.format(train_loss, val_loss))   \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    current_folder = globals()['_dh'][0]\n",
    "    path = os.path.dirname(os.path.join(current_folder,''))\n",
    "    os.chdir(path)\n",
    "    \n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    train_img_path = './images'\n",
    "    train_key_path = './filename.csv'\n",
    "\n",
    "    # Load the entire DataFrame and split it into parts.\n",
    "    total_df = pd.read_csv(train_key_path)\n",
    "    num_parts = 1\n",
    "    data_parts = np.array_split(total_df, num_parts)\n",
    "\n",
    "    model_path = None\n",
    "\n",
    "    # Train on each part sequentially.\n",
    "    for i in range(num_parts):\n",
    "        print(f\"Training on part {i + 1}/{num_parts}\")\n",
    "        train_key, valid_key = train_test_split(data_parts[i], test_size=0.3, random_state=42)\n",
    "        train_loader, valid_loader = load_data(train_img_path, train_key, valid_key)\n",
    "\n",
    "        model_path = f\"./models/RCNN_part{i + 1}.pt\"\n",
    "        train_model(train_loader, valid_loader, model_path=model_path, num_epochs=30, device=DEVICE)\n",
    "    '''\n",
    "    default: epoch - 30, \n",
    "             device - cuda\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38829795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Keypoint Loss (avg): 2.7222\n",
      "\n",
      "---------------------- epoch 29 ------------------------\n",
      "Train Keypoint Loss: 2.7222, Val Keypoint Loss: 1.7162\n",
      "Completed in 745m 53s\n",
      "\n",
      "| epoch: 30 | batch: 200/74328 | batch loss: 3.0117721557617188\n",
      "| epoch: 30 | batch: 400/74328 | batch loss: 2.0546228885650635\n",
      "| epoch: 30 | batch: 600/74328 | batch loss: 2.38338565826416\n",
      "| epoch: 30 | batch: 800/74328 | batch loss: 2.2934749126434326\n",
      "| epoch: 30 | batch: 1000/74328 | batch loss: 2.365713357925415\n",
      "| epoch: 30 | batch: 1200/74328 | batch loss: 2.466822385787964\n",
      "| epoch: 30 | batch: 1400/74328 | batch loss: 2.49348521232605\n",
      "| epoch: 30 | batch: 1600/74328 | batch loss: 2.356402635574341\n",
      "| epoch: 30 | batch: 1800/74328 | batch loss: 3.455949068069458\n",
      "| epoch: 30 | batch: 2000/74328 | batch loss: 2.2623848915100098\n",
      "| epoch: 30 | batch: 2200/74328 | batch loss: 2.1707465648651123\n",
      "| epoch: 30 | batch: 2400/74328 | batch loss: 3.1435694694519043\n",
      "| epoch: 30 | batch: 2600/74328 | batch loss: 1.978074073791504\n",
      "| epoch: 30 | batch: 2800/74328 | batch loss: 2.035172700881958\n",
      "| epoch: 30 | batch: 3000/74328 | batch loss: 2.917539119720459\n",
      "| epoch: 30 | batch: 3200/74328 | batch loss: 2.0671372413635254\n",
      "| epoch: 30 | batch: 3400/74328 | batch loss: 1.345826268196106\n",
      "| epoch: 30 | batch: 3600/74328 | batch loss: 2.2353298664093018\n",
      "| epoch: 30 | batch: 3800/74328 | batch loss: 2.1826577186584473\n",
      "| epoch: 30 | batch: 4000/74328 | batch loss: 3.033524990081787\n",
      "| epoch: 30 | batch: 4200/74328 | batch loss: 3.865561008453369\n",
      "| epoch: 30 | batch: 4400/74328 | batch loss: 1.857654094696045\n",
      "| epoch: 30 | batch: 4600/74328 | batch loss: 3.012528419494629\n",
      "| epoch: 30 | batch: 4800/74328 | batch loss: 2.498152732849121\n",
      "| epoch: 30 | batch: 5000/74328 | batch loss: 2.117004871368408\n",
      "| epoch: 30 | batch: 5200/74328 | batch loss: 2.272944927215576\n",
      "| epoch: 30 | batch: 5400/74328 | batch loss: 1.6294907331466675\n",
      "| epoch: 30 | batch: 5600/74328 | batch loss: 3.3716368675231934\n",
      "| epoch: 30 | batch: 5800/74328 | batch loss: 4.334006309509277\n",
      "| epoch: 30 | batch: 6000/74328 | batch loss: 2.429548740386963\n",
      "| epoch: 30 | batch: 6200/74328 | batch loss: 2.0311155319213867\n",
      "| epoch: 30 | batch: 6400/74328 | batch loss: 2.331228494644165\n",
      "| epoch: 30 | batch: 6600/74328 | batch loss: 2.100436210632324\n",
      "| epoch: 30 | batch: 6800/74328 | batch loss: 2.112700939178467\n",
      "| epoch: 30 | batch: 7000/74328 | batch loss: 3.111165761947632\n",
      "| epoch: 30 | batch: 7200/74328 | batch loss: 2.290555953979492\n",
      "| epoch: 30 | batch: 7400/74328 | batch loss: 2.614204168319702\n",
      "| epoch: 30 | batch: 7600/74328 | batch loss: 3.0884227752685547\n",
      "| epoch: 30 | batch: 7800/74328 | batch loss: 1.9842158555984497\n",
      "| epoch: 30 | batch: 8000/74328 | batch loss: 2.7084014415740967\n",
      "| epoch: 30 | batch: 8200/74328 | batch loss: 1.9886159896850586\n",
      "| epoch: 30 | batch: 8400/74328 | batch loss: 2.4561455249786377\n",
      "| epoch: 30 | batch: 8600/74328 | batch loss: 2.682302713394165\n",
      "| epoch: 30 | batch: 8800/74328 | batch loss: 2.14613938331604\n",
      "| epoch: 30 | batch: 9000/74328 | batch loss: 3.4758594036102295\n",
      "| epoch: 30 | batch: 9200/74328 | batch loss: 2.0230772495269775\n",
      "| epoch: 30 | batch: 9400/74328 | batch loss: 2.172823667526245\n",
      "| epoch: 30 | batch: 9600/74328 | batch loss: 3.0345351696014404\n",
      "| epoch: 30 | batch: 9800/74328 | batch loss: 2.1767468452453613\n",
      "| epoch: 30 | batch: 10000/74328 | batch loss: 3.5134122371673584\n",
      "| epoch: 30 | batch: 10200/74328 | batch loss: 2.5291593074798584\n",
      "| epoch: 30 | batch: 10400/74328 | batch loss: 2.4472875595092773\n",
      "| epoch: 30 | batch: 10600/74328 | batch loss: 2.1731343269348145\n",
      "| epoch: 30 | batch: 10800/74328 | batch loss: 2.246741771697998\n",
      "| epoch: 30 | batch: 11000/74328 | batch loss: 2.4121086597442627\n",
      "| epoch: 30 | batch: 11200/74328 | batch loss: 2.242483377456665\n",
      "| epoch: 30 | batch: 11400/74328 | batch loss: 1.9191336631774902\n",
      "| epoch: 30 | batch: 11600/74328 | batch loss: 2.5264735221862793\n",
      "| epoch: 30 | batch: 11800/74328 | batch loss: 2.8959262371063232\n",
      "| epoch: 30 | batch: 12000/74328 | batch loss: 2.042100667953491\n",
      "| epoch: 30 | batch: 12200/74328 | batch loss: 3.7603187561035156\n",
      "| epoch: 30 | batch: 12400/74328 | batch loss: 2.393989086151123\n",
      "| epoch: 30 | batch: 12600/74328 | batch loss: 3.379863977432251\n",
      "| epoch: 30 | batch: 12800/74328 | batch loss: 3.5601184368133545\n",
      "| epoch: 30 | batch: 13000/74328 | batch loss: 3.155181884765625\n",
      "| epoch: 30 | batch: 13200/74328 | batch loss: 2.141106605529785\n",
      "| epoch: 30 | batch: 13400/74328 | batch loss: 2.2273988723754883\n",
      "| epoch: 30 | batch: 13600/74328 | batch loss: 4.088260650634766\n",
      "| epoch: 30 | batch: 13800/74328 | batch loss: 3.9738752841949463\n",
      "| epoch: 30 | batch: 14000/74328 | batch loss: 2.6644716262817383\n",
      "| epoch: 30 | batch: 14200/74328 | batch loss: 1.5058677196502686\n",
      "| epoch: 30 | batch: 14400/74328 | batch loss: 3.7368316650390625\n",
      "| epoch: 30 | batch: 14600/74328 | batch loss: 2.8213939666748047\n",
      "| epoch: 30 | batch: 14800/74328 | batch loss: 2.4626951217651367\n",
      "| epoch: 30 | batch: 15000/74328 | batch loss: 2.8339717388153076\n",
      "| epoch: 30 | batch: 15200/74328 | batch loss: 2.5640013217926025\n",
      "| epoch: 30 | batch: 15400/74328 | batch loss: 2.4324839115142822\n",
      "| epoch: 30 | batch: 15600/74328 | batch loss: 1.5166938304901123\n",
      "| epoch: 30 | batch: 15800/74328 | batch loss: 2.8130598068237305\n",
      "| epoch: 30 | batch: 16000/74328 | batch loss: 2.285194158554077\n",
      "| epoch: 30 | batch: 16200/74328 | batch loss: 4.311686038970947\n",
      "| epoch: 30 | batch: 16400/74328 | batch loss: 3.063797950744629\n",
      "| epoch: 30 | batch: 16600/74328 | batch loss: 2.596989154815674\n",
      "| epoch: 30 | batch: 16800/74328 | batch loss: 2.4346210956573486\n",
      "| epoch: 30 | batch: 17000/74328 | batch loss: 2.9145822525024414\n",
      "| epoch: 30 | batch: 17200/74328 | batch loss: 2.565234661102295\n",
      "| epoch: 30 | batch: 17400/74328 | batch loss: 2.981019973754883\n",
      "| epoch: 30 | batch: 17600/74328 | batch loss: 3.687439203262329\n",
      "| epoch: 30 | batch: 17800/74328 | batch loss: 2.4613542556762695\n",
      "| epoch: 30 | batch: 18000/74328 | batch loss: 2.748908519744873\n",
      "| epoch: 30 | batch: 18200/74328 | batch loss: 3.0303890705108643\n",
      "| epoch: 30 | batch: 18400/74328 | batch loss: 2.009636163711548\n",
      "| epoch: 30 | batch: 18600/74328 | batch loss: 1.9267529249191284\n",
      "| epoch: 30 | batch: 18800/74328 | batch loss: 3.3853676319122314\n",
      "| epoch: 30 | batch: 19000/74328 | batch loss: 2.805629253387451\n",
      "| epoch: 30 | batch: 19200/74328 | batch loss: 2.387387990951538\n",
      "| epoch: 30 | batch: 19400/74328 | batch loss: 3.327162742614746\n",
      "| epoch: 30 | batch: 19600/74328 | batch loss: 2.9015402793884277\n",
      "| epoch: 30 | batch: 19800/74328 | batch loss: 2.5031516551971436\n",
      "| epoch: 30 | batch: 20000/74328 | batch loss: 3.844489097595215\n",
      "| epoch: 30 | batch: 20200/74328 | batch loss: 2.589866876602173\n",
      "| epoch: 30 | batch: 20400/74328 | batch loss: 3.2016425132751465\n",
      "| epoch: 30 | batch: 20600/74328 | batch loss: 2.912607431411743\n",
      "| epoch: 30 | batch: 20800/74328 | batch loss: 2.091566562652588\n",
      "| epoch: 30 | batch: 21000/74328 | batch loss: 2.53413987159729\n",
      "| epoch: 30 | batch: 21200/74328 | batch loss: 3.8654227256774902\n",
      "| epoch: 30 | batch: 21400/74328 | batch loss: 2.74595308303833\n",
      "| epoch: 30 | batch: 21600/74328 | batch loss: 1.7983850240707397\n",
      "| epoch: 30 | batch: 21800/74328 | batch loss: 2.750152826309204\n",
      "| epoch: 30 | batch: 22000/74328 | batch loss: 1.8444498777389526\n",
      "| epoch: 30 | batch: 22200/74328 | batch loss: 2.806104898452759\n",
      "| epoch: 30 | batch: 22400/74328 | batch loss: 2.3994340896606445\n",
      "| epoch: 30 | batch: 22600/74328 | batch loss: 2.8892276287078857\n",
      "| epoch: 30 | batch: 22800/74328 | batch loss: 2.5010836124420166\n",
      "| epoch: 30 | batch: 23000/74328 | batch loss: 2.560393810272217\n",
      "| epoch: 30 | batch: 23200/74328 | batch loss: 3.8608431816101074\n",
      "| epoch: 30 | batch: 23400/74328 | batch loss: 2.2221906185150146\n",
      "| epoch: 30 | batch: 23600/74328 | batch loss: 2.6059787273406982\n",
      "| epoch: 30 | batch: 23800/74328 | batch loss: 2.4959073066711426\n",
      "| epoch: 30 | batch: 24000/74328 | batch loss: 1.7748301029205322\n",
      "| epoch: 30 | batch: 24200/74328 | batch loss: 2.1379494667053223\n",
      "| epoch: 30 | batch: 24400/74328 | batch loss: 2.2478508949279785\n",
      "| epoch: 30 | batch: 24600/74328 | batch loss: 2.7402334213256836\n",
      "| epoch: 30 | batch: 24800/74328 | batch loss: 2.311677932739258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 30 | batch: 25000/74328 | batch loss: 2.418539047241211\n",
      "| epoch: 30 | batch: 25200/74328 | batch loss: 2.69354510307312\n",
      "| epoch: 30 | batch: 25400/74328 | batch loss: 2.960627794265747\n",
      "| epoch: 30 | batch: 25600/74328 | batch loss: 2.3154726028442383\n",
      "| epoch: 30 | batch: 25800/74328 | batch loss: 2.810124397277832\n",
      "| epoch: 30 | batch: 26000/74328 | batch loss: 4.4683732986450195\n",
      "| epoch: 30 | batch: 26200/74328 | batch loss: 2.283747673034668\n",
      "| epoch: 30 | batch: 26400/74328 | batch loss: 2.493077039718628\n",
      "| epoch: 30 | batch: 26600/74328 | batch loss: 2.306143283843994\n",
      "| epoch: 30 | batch: 26800/74328 | batch loss: 2.80708384513855\n",
      "| epoch: 30 | batch: 27000/74328 | batch loss: 2.955204486846924\n",
      "| epoch: 30 | batch: 27200/74328 | batch loss: 1.8457856178283691\n",
      "| epoch: 30 | batch: 27400/74328 | batch loss: 2.534271478652954\n",
      "| epoch: 30 | batch: 27600/74328 | batch loss: 3.5484910011291504\n",
      "| epoch: 30 | batch: 27800/74328 | batch loss: 1.9277796745300293\n",
      "| epoch: 30 | batch: 28000/74328 | batch loss: 2.353161334991455\n",
      "| epoch: 30 | batch: 28200/74328 | batch loss: 3.1789321899414062\n",
      "| epoch: 30 | batch: 28400/74328 | batch loss: 2.760833978652954\n",
      "| epoch: 30 | batch: 28600/74328 | batch loss: 2.445490598678589\n",
      "| epoch: 30 | batch: 28800/74328 | batch loss: 2.7566566467285156\n",
      "| epoch: 30 | batch: 29000/74328 | batch loss: 2.6285347938537598\n",
      "| epoch: 30 | batch: 29200/74328 | batch loss: 4.262816429138184\n",
      "| epoch: 30 | batch: 29400/74328 | batch loss: 2.798236131668091\n",
      "| epoch: 30 | batch: 29600/74328 | batch loss: 3.0568015575408936\n",
      "| epoch: 30 | batch: 29800/74328 | batch loss: 2.4632513523101807\n",
      "| epoch: 30 | batch: 30000/74328 | batch loss: 2.998206377029419\n",
      "| epoch: 30 | batch: 30200/74328 | batch loss: 2.3103396892547607\n",
      "| epoch: 30 | batch: 30400/74328 | batch loss: 2.649291753768921\n",
      "| epoch: 30 | batch: 30600/74328 | batch loss: 2.276745557785034\n",
      "| epoch: 30 | batch: 30800/74328 | batch loss: 2.622758388519287\n",
      "| epoch: 30 | batch: 31000/74328 | batch loss: 2.8935534954071045\n",
      "| epoch: 30 | batch: 31200/74328 | batch loss: 2.4162192344665527\n",
      "| epoch: 30 | batch: 31400/74328 | batch loss: 2.873087167739868\n",
      "| epoch: 30 | batch: 31600/74328 | batch loss: 3.605175495147705\n",
      "| epoch: 30 | batch: 31800/74328 | batch loss: 2.4120612144470215\n",
      "| epoch: 30 | batch: 32000/74328 | batch loss: 2.8098232746124268\n",
      "| epoch: 30 | batch: 32200/74328 | batch loss: 2.59901762008667\n",
      "| epoch: 30 | batch: 32400/74328 | batch loss: 2.5358574390411377\n",
      "| epoch: 30 | batch: 32600/74328 | batch loss: 4.225921154022217\n",
      "| epoch: 30 | batch: 32800/74328 | batch loss: 2.619584798812866\n",
      "| epoch: 30 | batch: 33000/74328 | batch loss: 2.8885345458984375\n",
      "| epoch: 30 | batch: 33200/74328 | batch loss: 3.4805266857147217\n",
      "| epoch: 30 | batch: 33400/74328 | batch loss: 3.304138660430908\n",
      "| epoch: 30 | batch: 33600/74328 | batch loss: 1.653211236000061\n",
      "| epoch: 30 | batch: 33800/74328 | batch loss: 1.6834475994110107\n",
      "| epoch: 30 | batch: 34000/74328 | batch loss: 2.7352871894836426\n",
      "| epoch: 30 | batch: 34200/74328 | batch loss: 3.8574466705322266\n",
      "| epoch: 30 | batch: 34400/74328 | batch loss: 3.1994435787200928\n",
      "| epoch: 30 | batch: 34600/74328 | batch loss: 1.6653573513031006\n",
      "| epoch: 30 | batch: 34800/74328 | batch loss: 1.953602910041809\n",
      "| epoch: 30 | batch: 35000/74328 | batch loss: 2.6081857681274414\n",
      "| epoch: 30 | batch: 35200/74328 | batch loss: 4.718423843383789\n",
      "| epoch: 30 | batch: 35400/74328 | batch loss: 3.7072596549987793\n",
      "| epoch: 30 | batch: 35600/74328 | batch loss: 2.94423770904541\n",
      "| epoch: 30 | batch: 35800/74328 | batch loss: 3.0157766342163086\n",
      "| epoch: 30 | batch: 36000/74328 | batch loss: 3.097289562225342\n",
      "| epoch: 30 | batch: 36200/74328 | batch loss: 2.633241891860962\n",
      "| epoch: 30 | batch: 36400/74328 | batch loss: 3.436049461364746\n",
      "| epoch: 30 | batch: 36600/74328 | batch loss: 2.442795753479004\n",
      "| epoch: 30 | batch: 36800/74328 | batch loss: 2.781135082244873\n",
      "| epoch: 30 | batch: 37000/74328 | batch loss: 3.059169292449951\n",
      "| epoch: 30 | batch: 37200/74328 | batch loss: 2.458061933517456\n",
      "| epoch: 30 | batch: 37400/74328 | batch loss: 2.983260154724121\n",
      "| epoch: 30 | batch: 37600/74328 | batch loss: 2.198580265045166\n",
      "| epoch: 30 | batch: 37800/74328 | batch loss: 2.8521499633789062\n",
      "| epoch: 30 | batch: 38000/74328 | batch loss: 2.5123276710510254\n",
      "| epoch: 30 | batch: 38200/74328 | batch loss: 2.4324517250061035\n",
      "| epoch: 30 | batch: 38400/74328 | batch loss: 3.0168709754943848\n",
      "| epoch: 30 | batch: 38600/74328 | batch loss: 4.17882776260376\n",
      "| epoch: 30 | batch: 38800/74328 | batch loss: 4.210320949554443\n",
      "| epoch: 30 | batch: 39000/74328 | batch loss: 2.4971444606781006\n",
      "| epoch: 30 | batch: 39200/74328 | batch loss: 3.456822156906128\n",
      "| epoch: 30 | batch: 39400/74328 | batch loss: 2.475320339202881\n",
      "| epoch: 30 | batch: 39600/74328 | batch loss: 2.511672019958496\n",
      "| epoch: 30 | batch: 39800/74328 | batch loss: 3.1424784660339355\n",
      "| epoch: 30 | batch: 40000/74328 | batch loss: 2.166893482208252\n",
      "| epoch: 30 | batch: 40200/74328 | batch loss: 1.9914995431900024\n",
      "| epoch: 30 | batch: 40400/74328 | batch loss: 2.9673452377319336\n",
      "| epoch: 30 | batch: 40600/74328 | batch loss: 3.1746065616607666\n",
      "| epoch: 30 | batch: 40800/74328 | batch loss: 3.3080639839172363\n",
      "| epoch: 30 | batch: 41000/74328 | batch loss: 2.3533480167388916\n",
      "| epoch: 30 | batch: 41200/74328 | batch loss: 2.6017849445343018\n",
      "| epoch: 30 | batch: 41400/74328 | batch loss: 2.446207046508789\n",
      "| epoch: 30 | batch: 41600/74328 | batch loss: 1.921700358390808\n",
      "| epoch: 30 | batch: 41800/74328 | batch loss: 2.1240522861480713\n",
      "| epoch: 30 | batch: 42000/74328 | batch loss: 2.2498879432678223\n",
      "| epoch: 30 | batch: 42200/74328 | batch loss: 2.5402984619140625\n",
      "| epoch: 30 | batch: 42400/74328 | batch loss: 2.626183271408081\n",
      "| epoch: 30 | batch: 42600/74328 | batch loss: 3.327542304992676\n",
      "| epoch: 30 | batch: 42800/74328 | batch loss: 3.078611135482788\n",
      "| epoch: 30 | batch: 43000/74328 | batch loss: 2.5888307094573975\n",
      "| epoch: 30 | batch: 43200/74328 | batch loss: 3.312990188598633\n",
      "| epoch: 30 | batch: 43400/74328 | batch loss: 2.275165557861328\n",
      "| epoch: 30 | batch: 43600/74328 | batch loss: 2.991586685180664\n",
      "| epoch: 30 | batch: 43800/74328 | batch loss: 3.2889091968536377\n",
      "| epoch: 30 | batch: 44000/74328 | batch loss: 2.4258995056152344\n",
      "| epoch: 30 | batch: 44200/74328 | batch loss: 1.8391064405441284\n",
      "| epoch: 30 | batch: 44400/74328 | batch loss: 2.272491693496704\n",
      "| epoch: 30 | batch: 44600/74328 | batch loss: 1.864463210105896\n",
      "| epoch: 30 | batch: 44800/74328 | batch loss: 2.110868215560913\n",
      "| epoch: 30 | batch: 45000/74328 | batch loss: 1.966757893562317\n",
      "| epoch: 30 | batch: 45200/74328 | batch loss: 3.365602493286133\n",
      "| epoch: 30 | batch: 45400/74328 | batch loss: 3.462509870529175\n",
      "| epoch: 30 | batch: 45600/74328 | batch loss: 2.5836901664733887\n",
      "| epoch: 30 | batch: 45800/74328 | batch loss: 2.4821457862854004\n",
      "| epoch: 30 | batch: 46000/74328 | batch loss: 4.035677909851074\n",
      "| epoch: 30 | batch: 46200/74328 | batch loss: 2.8308935165405273\n",
      "| epoch: 30 | batch: 46400/74328 | batch loss: 2.4191505908966064\n",
      "| epoch: 30 | batch: 46600/74328 | batch loss: 2.2255067825317383\n",
      "| epoch: 30 | batch: 46800/74328 | batch loss: 3.776764392852783\n",
      "| epoch: 30 | batch: 47000/74328 | batch loss: 2.307417154312134\n",
      "| epoch: 30 | batch: 47200/74328 | batch loss: 2.6278634071350098\n",
      "| epoch: 30 | batch: 47400/74328 | batch loss: 2.7269651889801025\n",
      "| epoch: 30 | batch: 47600/74328 | batch loss: 2.791013717651367\n",
      "| epoch: 30 | batch: 47800/74328 | batch loss: 2.4164366722106934\n",
      "| epoch: 30 | batch: 48000/74328 | batch loss: 2.8247909545898438\n",
      "| epoch: 30 | batch: 48200/74328 | batch loss: 2.819106101989746\n",
      "| epoch: 30 | batch: 48400/74328 | batch loss: 3.2304117679595947\n",
      "| epoch: 30 | batch: 48600/74328 | batch loss: 3.571305990219116\n",
      "| epoch: 30 | batch: 48800/74328 | batch loss: 2.750347375869751\n",
      "| epoch: 30 | batch: 49000/74328 | batch loss: 2.6638975143432617\n",
      "| epoch: 30 | batch: 49200/74328 | batch loss: 2.134324073791504\n",
      "| epoch: 30 | batch: 49400/74328 | batch loss: 4.8662824630737305\n",
      "| epoch: 30 | batch: 49600/74328 | batch loss: 3.2327933311462402\n",
      "| epoch: 30 | batch: 49800/74328 | batch loss: 2.9675278663635254\n",
      "| epoch: 30 | batch: 50000/74328 | batch loss: 3.889918804168701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 30 | batch: 50200/74328 | batch loss: 3.3694891929626465\n",
      "| epoch: 30 | batch: 50400/74328 | batch loss: 2.9172677993774414\n",
      "| epoch: 30 | batch: 50600/74328 | batch loss: 3.7928881645202637\n",
      "| epoch: 30 | batch: 50800/74328 | batch loss: 2.5667877197265625\n",
      "| epoch: 30 | batch: 51000/74328 | batch loss: 3.6441967487335205\n",
      "| epoch: 30 | batch: 51200/74328 | batch loss: 2.204216241836548\n",
      "| epoch: 30 | batch: 51400/74328 | batch loss: 2.278938055038452\n",
      "| epoch: 30 | batch: 51600/74328 | batch loss: 1.7713336944580078\n",
      "| epoch: 30 | batch: 51800/74328 | batch loss: 1.9769757986068726\n",
      "| epoch: 30 | batch: 52000/74328 | batch loss: 1.968371033668518\n",
      "| epoch: 30 | batch: 52200/74328 | batch loss: 2.9566643238067627\n",
      "| epoch: 30 | batch: 52400/74328 | batch loss: 4.360531330108643\n",
      "| epoch: 30 | batch: 52600/74328 | batch loss: 2.651684522628784\n",
      "| epoch: 30 | batch: 52800/74328 | batch loss: 2.54831862449646\n",
      "| epoch: 30 | batch: 53000/74328 | batch loss: 2.555910348892212\n",
      "| epoch: 30 | batch: 53200/74328 | batch loss: 2.868386745452881\n",
      "| epoch: 30 | batch: 53400/74328 | batch loss: 1.8426225185394287\n",
      "| epoch: 30 | batch: 53600/74328 | batch loss: 2.150275230407715\n",
      "| epoch: 30 | batch: 53800/74328 | batch loss: 3.103879451751709\n",
      "| epoch: 30 | batch: 54000/74328 | batch loss: 3.319080114364624\n",
      "| epoch: 30 | batch: 54200/74328 | batch loss: 2.3171515464782715\n",
      "| epoch: 30 | batch: 54400/74328 | batch loss: 3.235112190246582\n",
      "| epoch: 30 | batch: 54600/74328 | batch loss: 2.9538567066192627\n",
      "| epoch: 30 | batch: 54800/74328 | batch loss: 1.7758972644805908\n",
      "| epoch: 30 | batch: 55000/74328 | batch loss: 2.412631034851074\n",
      "| epoch: 30 | batch: 55200/74328 | batch loss: 3.5219132900238037\n",
      "| epoch: 30 | batch: 55400/74328 | batch loss: 3.1213982105255127\n",
      "| epoch: 30 | batch: 55600/74328 | batch loss: 2.4053773880004883\n",
      "| epoch: 30 | batch: 55800/74328 | batch loss: 2.943138360977173\n",
      "| epoch: 30 | batch: 56000/74328 | batch loss: 2.5127861499786377\n",
      "| epoch: 30 | batch: 56200/74328 | batch loss: 3.479468822479248\n",
      "| epoch: 30 | batch: 56400/74328 | batch loss: 1.986463189125061\n",
      "| epoch: 30 | batch: 56600/74328 | batch loss: 2.6905033588409424\n",
      "| epoch: 30 | batch: 56800/74328 | batch loss: 3.350069522857666\n",
      "| epoch: 30 | batch: 57000/74328 | batch loss: 2.5736169815063477\n",
      "| epoch: 30 | batch: 57200/74328 | batch loss: 2.8137497901916504\n",
      "| epoch: 30 | batch: 57400/74328 | batch loss: 2.709150552749634\n",
      "| epoch: 30 | batch: 57600/74328 | batch loss: 2.6472952365875244\n",
      "| epoch: 30 | batch: 57800/74328 | batch loss: 2.583782196044922\n",
      "| epoch: 30 | batch: 58000/74328 | batch loss: 2.7597413063049316\n",
      "| epoch: 30 | batch: 58200/74328 | batch loss: 2.286810874938965\n",
      "| epoch: 30 | batch: 58400/74328 | batch loss: 3.3270211219787598\n",
      "| epoch: 30 | batch: 58600/74328 | batch loss: 4.721242904663086\n",
      "| epoch: 30 | batch: 58800/74328 | batch loss: 2.9896607398986816\n",
      "| epoch: 30 | batch: 59000/74328 | batch loss: 2.751042127609253\n",
      "| epoch: 30 | batch: 59200/74328 | batch loss: 2.4754693508148193\n",
      "| epoch: 30 | batch: 59400/74328 | batch loss: 2.5551328659057617\n",
      "| epoch: 30 | batch: 59600/74328 | batch loss: 2.6365408897399902\n",
      "| epoch: 30 | batch: 59800/74328 | batch loss: 2.8414297103881836\n",
      "| epoch: 30 | batch: 60000/74328 | batch loss: 2.9400217533111572\n",
      "| epoch: 30 | batch: 60200/74328 | batch loss: 3.427288770675659\n",
      "| epoch: 30 | batch: 60400/74328 | batch loss: 2.557312250137329\n",
      "| epoch: 30 | batch: 60600/74328 | batch loss: 2.316317081451416\n",
      "| epoch: 30 | batch: 60800/74328 | batch loss: 4.16939640045166\n",
      "| epoch: 30 | batch: 61000/74328 | batch loss: 3.8074967861175537\n",
      "| epoch: 30 | batch: 61200/74328 | batch loss: 3.099348545074463\n",
      "| epoch: 30 | batch: 61400/74328 | batch loss: 2.66559100151062\n",
      "| epoch: 30 | batch: 61600/74328 | batch loss: 2.3016152381896973\n",
      "| epoch: 30 | batch: 61800/74328 | batch loss: 2.5355257987976074\n",
      "| epoch: 30 | batch: 62000/74328 | batch loss: 3.2085683345794678\n",
      "| epoch: 30 | batch: 62200/74328 | batch loss: 1.86918306350708\n",
      "| epoch: 30 | batch: 62400/74328 | batch loss: 3.157696008682251\n",
      "| epoch: 30 | batch: 62600/74328 | batch loss: 3.4126172065734863\n",
      "| epoch: 30 | batch: 62800/74328 | batch loss: 3.4425768852233887\n",
      "| epoch: 30 | batch: 63000/74328 | batch loss: 1.839035987854004\n",
      "| epoch: 30 | batch: 63200/74328 | batch loss: 2.0855910778045654\n",
      "| epoch: 30 | batch: 63400/74328 | batch loss: 2.599369764328003\n",
      "| epoch: 30 | batch: 63600/74328 | batch loss: 3.2656140327453613\n",
      "| epoch: 30 | batch: 63800/74328 | batch loss: 2.8198983669281006\n",
      "| epoch: 30 | batch: 64000/74328 | batch loss: 2.3875484466552734\n",
      "| epoch: 30 | batch: 64200/74328 | batch loss: 2.2303285598754883\n",
      "| epoch: 30 | batch: 64400/74328 | batch loss: 2.761408805847168\n",
      "| epoch: 30 | batch: 64600/74328 | batch loss: 2.5013468265533447\n",
      "| epoch: 30 | batch: 64800/74328 | batch loss: 2.45953369140625\n",
      "| epoch: 30 | batch: 65000/74328 | batch loss: 1.9915400743484497\n",
      "| epoch: 30 | batch: 65200/74328 | batch loss: 3.5276002883911133\n",
      "| epoch: 30 | batch: 65400/74328 | batch loss: 2.56288743019104\n",
      "| epoch: 30 | batch: 65600/74328 | batch loss: 2.6951239109039307\n",
      "| epoch: 30 | batch: 65800/74328 | batch loss: 3.147891044616699\n",
      "| epoch: 30 | batch: 66000/74328 | batch loss: 4.369615077972412\n",
      "| epoch: 30 | batch: 66200/74328 | batch loss: 3.1491920948028564\n",
      "| epoch: 30 | batch: 66400/74328 | batch loss: 2.4393632411956787\n",
      "| epoch: 30 | batch: 66600/74328 | batch loss: 2.39225435256958\n",
      "| epoch: 30 | batch: 66800/74328 | batch loss: 3.097444534301758\n",
      "| epoch: 30 | batch: 67000/74328 | batch loss: 4.1266679763793945\n",
      "| epoch: 30 | batch: 67200/74328 | batch loss: 2.0493006706237793\n",
      "| epoch: 30 | batch: 67400/74328 | batch loss: 1.8952467441558838\n",
      "| epoch: 30 | batch: 67600/74328 | batch loss: 3.366878032684326\n",
      "| epoch: 30 | batch: 67800/74328 | batch loss: 2.5903265476226807\n",
      "| epoch: 30 | batch: 68000/74328 | batch loss: 2.7815442085266113\n",
      "| epoch: 30 | batch: 68200/74328 | batch loss: 2.858586311340332\n",
      "| epoch: 30 | batch: 68400/74328 | batch loss: 2.5483357906341553\n",
      "| epoch: 30 | batch: 68600/74328 | batch loss: 2.40799617767334\n",
      "| epoch: 30 | batch: 68800/74328 | batch loss: 2.957798480987549\n",
      "| epoch: 30 | batch: 69000/74328 | batch loss: 2.755990505218506\n",
      "| epoch: 30 | batch: 69200/74328 | batch loss: 2.771742105484009\n",
      "| epoch: 30 | batch: 69400/74328 | batch loss: 2.6764261722564697\n",
      "| epoch: 30 | batch: 69600/74328 | batch loss: 2.652970790863037\n",
      "| epoch: 30 | batch: 69800/74328 | batch loss: 2.551081895828247\n",
      "| epoch: 30 | batch: 70000/74328 | batch loss: 2.7045278549194336\n",
      "| epoch: 30 | batch: 70200/74328 | batch loss: 2.6178975105285645\n",
      "| epoch: 30 | batch: 70400/74328 | batch loss: 3.249588966369629\n",
      "| epoch: 30 | batch: 70600/74328 | batch loss: 3.0017828941345215\n",
      "| epoch: 30 | batch: 70800/74328 | batch loss: 2.004152774810791\n",
      "| epoch: 30 | batch: 71000/74328 | batch loss: 2.0815587043762207\n",
      "| epoch: 30 | batch: 71200/74328 | batch loss: 3.2073535919189453\n",
      "| epoch: 30 | batch: 71400/74328 | batch loss: 2.8987910747528076\n",
      "| epoch: 30 | batch: 71600/74328 | batch loss: 2.535644054412842\n",
      "| epoch: 30 | batch: 71800/74328 | batch loss: 2.2803783416748047\n",
      "| epoch: 30 | batch: 72000/74328 | batch loss: 2.850749969482422\n",
      "| epoch: 30 | batch: 72200/74328 | batch loss: 2.223278760910034\n",
      "| epoch: 30 | batch: 72400/74328 | batch loss: 3.0535922050476074\n",
      "| epoch: 30 | batch: 72600/74328 | batch loss: 2.712585926055908\n",
      "| epoch: 30 | batch: 72800/74328 | batch loss: 2.9218950271606445\n",
      "| epoch: 30 | batch: 73000/74328 | batch loss: 2.8359298706054688\n",
      "| epoch: 30 | batch: 73200/74328 | batch loss: 2.2973403930664062\n",
      "| epoch: 30 | batch: 73400/74328 | batch loss: 2.22819447517395\n",
      "| epoch: 30 | batch: 73600/74328 | batch loss: 3.650723934173584\n",
      "| epoch: 30 | batch: 73800/74328 | batch loss: 2.844247341156006\n",
      "| epoch: 30 | batch: 74000/74328 | batch loss: 2.507120132446289\n",
      "| epoch: 30 | batch: 74200/74328 | batch loss: 2.307886838912964\n",
      "| epoch: 30 | batch: 200/74328 | batch loss: 2.068789005279541\n",
      "| epoch: 30 | batch: 400/74328 | batch loss: 3.6706464290618896\n",
      "| epoch: 30 | batch: 600/74328 | batch loss: 2.7111129760742188\n",
      "| epoch: 30 | batch: 800/74328 | batch loss: 2.237370014190674\n",
      "| epoch: 30 | batch: 1000/74328 | batch loss: 1.7275727987289429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 30 | batch: 1200/74328 | batch loss: 2.936589002609253\n",
      "| epoch: 30 | batch: 1400/74328 | batch loss: 2.567556381225586\n",
      "| epoch: 30 | batch: 1600/74328 | batch loss: 2.390641450881958\n",
      "| epoch: 30 | batch: 1800/74328 | batch loss: 2.333359956741333\n",
      "| epoch: 30 | batch: 2000/74328 | batch loss: 3.331517457962036\n",
      "| epoch: 30 | batch: 2200/74328 | batch loss: 1.829755187034607\n",
      "| epoch: 30 | batch: 2400/74328 | batch loss: 2.6339423656463623\n",
      "| epoch: 30 | batch: 2600/74328 | batch loss: 2.5312702655792236\n",
      "| epoch: 30 | batch: 2800/74328 | batch loss: 2.803135395050049\n",
      "| epoch: 30 | batch: 3000/74328 | batch loss: 3.899508476257324\n",
      "| epoch: 30 | batch: 3200/74328 | batch loss: 2.845576763153076\n",
      "| epoch: 30 | batch: 3400/74328 | batch loss: 2.3201980590820312\n",
      "| epoch: 30 | batch: 3600/74328 | batch loss: 2.7210001945495605\n",
      "| epoch: 30 | batch: 3800/74328 | batch loss: 2.6502623558044434\n",
      "| epoch: 30 | batch: 4000/74328 | batch loss: 2.235126495361328\n",
      "| epoch: 30 | batch: 4200/74328 | batch loss: 3.384653329849243\n",
      "| epoch: 30 | batch: 4400/74328 | batch loss: 3.9482150077819824\n",
      "| epoch: 30 | batch: 4600/74328 | batch loss: 2.8624227046966553\n",
      "| epoch: 30 | batch: 4800/74328 | batch loss: 2.0190656185150146\n",
      "| epoch: 30 | batch: 5000/74328 | batch loss: 2.056769371032715\n",
      "| epoch: 30 | batch: 5200/74328 | batch loss: 2.437958240509033\n",
      "| epoch: 30 | batch: 5400/74328 | batch loss: 2.1129777431488037\n",
      "| epoch: 30 | batch: 5600/74328 | batch loss: 1.5429117679595947\n",
      "| epoch: 30 | batch: 5800/74328 | batch loss: 2.790863513946533\n",
      "| epoch: 30 | batch: 6000/74328 | batch loss: 1.896867275238037\n",
      "| epoch: 30 | batch: 6200/74328 | batch loss: 2.026402711868286\n",
      "| epoch: 30 | batch: 6400/74328 | batch loss: 2.262345790863037\n",
      "| epoch: 30 | batch: 6600/74328 | batch loss: 2.4985978603363037\n",
      "| epoch: 30 | batch: 6800/74328 | batch loss: 2.3294050693511963\n",
      "| epoch: 30 | batch: 7000/74328 | batch loss: 1.9053715467453003\n",
      "| epoch: 30 | batch: 7200/74328 | batch loss: 2.889972448348999\n",
      "| epoch: 30 | batch: 7400/74328 | batch loss: 2.401796817779541\n",
      "| epoch: 30 | batch: 7600/74328 | batch loss: 2.153196096420288\n",
      "| epoch: 30 | batch: 7800/74328 | batch loss: 2.2228732109069824\n",
      "| epoch: 30 | batch: 8000/74328 | batch loss: 2.3454716205596924\n",
      "| epoch: 30 | batch: 8200/74328 | batch loss: 3.4766106605529785\n",
      "| epoch: 30 | batch: 8400/74328 | batch loss: 2.7146599292755127\n",
      "| epoch: 30 | batch: 8600/74328 | batch loss: 2.8719441890716553\n",
      "| epoch: 30 | batch: 8800/74328 | batch loss: 2.082012891769409\n",
      "| epoch: 30 | batch: 9000/74328 | batch loss: 1.9384571313858032\n",
      "| epoch: 30 | batch: 9200/74328 | batch loss: 2.2723610401153564\n",
      "| epoch: 30 | batch: 9400/74328 | batch loss: 1.9591535329818726\n",
      "| epoch: 30 | batch: 9600/74328 | batch loss: 2.6000375747680664\n",
      "| epoch: 30 | batch: 9800/74328 | batch loss: 2.0637359619140625\n",
      "| epoch: 30 | batch: 10000/74328 | batch loss: 2.221406936645508\n",
      "| epoch: 30 | batch: 10200/74328 | batch loss: 2.9193944931030273\n",
      "| epoch: 30 | batch: 10400/74328 | batch loss: 2.2222814559936523\n",
      "| epoch: 30 | batch: 10600/74328 | batch loss: 1.7417672872543335\n",
      "| epoch: 30 | batch: 10800/74328 | batch loss: 2.5232906341552734\n",
      "| epoch: 30 | batch: 11000/74328 | batch loss: 2.6783058643341064\n",
      "| epoch: 30 | batch: 11200/74328 | batch loss: 1.8980154991149902\n",
      "| epoch: 30 | batch: 11400/74328 | batch loss: 2.470290184020996\n",
      "| epoch: 30 | batch: 11600/74328 | batch loss: 2.8057854175567627\n",
      "| epoch: 30 | batch: 11800/74328 | batch loss: 2.552874803543091\n",
      "| epoch: 30 | batch: 12000/74328 | batch loss: 2.0824546813964844\n",
      "| epoch: 30 | batch: 12200/74328 | batch loss: 2.245662212371826\n",
      "| epoch: 30 | batch: 12400/74328 | batch loss: 3.637005567550659\n",
      "| epoch: 30 | batch: 12600/74328 | batch loss: 3.8732810020446777\n",
      "| epoch: 30 | batch: 12800/74328 | batch loss: 2.264080762863159\n",
      "| epoch: 30 | batch: 13000/74328 | batch loss: 2.3626010417938232\n",
      "| epoch: 30 | batch: 13200/74328 | batch loss: 2.612037420272827\n",
      "| epoch: 30 | batch: 13400/74328 | batch loss: 3.518597364425659\n",
      "| epoch: 30 | batch: 13600/74328 | batch loss: 2.1396329402923584\n",
      "| epoch: 30 | batch: 13800/74328 | batch loss: 2.808458089828491\n",
      "| epoch: 30 | batch: 14000/74328 | batch loss: 2.9888250827789307\n",
      "| epoch: 30 | batch: 14200/74328 | batch loss: 3.1931376457214355\n",
      "| epoch: 30 | batch: 14400/74328 | batch loss: 2.182736873626709\n",
      "| epoch: 30 | batch: 14600/74328 | batch loss: 2.7273688316345215\n",
      "| epoch: 30 | batch: 14800/74328 | batch loss: 1.7569246292114258\n",
      "| epoch: 30 | batch: 15000/74328 | batch loss: 2.9213709831237793\n",
      "| epoch: 30 | batch: 15200/74328 | batch loss: 1.8443620204925537\n",
      "| epoch: 30 | batch: 15400/74328 | batch loss: 3.2641825675964355\n",
      "| epoch: 30 | batch: 15600/74328 | batch loss: 1.991761565208435\n",
      "| epoch: 30 | batch: 15800/74328 | batch loss: 1.77596116065979\n",
      "| epoch: 30 | batch: 16000/74328 | batch loss: 2.6313486099243164\n",
      "| epoch: 30 | batch: 16200/74328 | batch loss: 1.7701013088226318\n",
      "| epoch: 30 | batch: 16400/74328 | batch loss: 2.3830037117004395\n",
      "| epoch: 30 | batch: 16600/74328 | batch loss: 3.5441253185272217\n",
      "| epoch: 30 | batch: 16800/74328 | batch loss: 2.317058801651001\n",
      "| epoch: 30 | batch: 17000/74328 | batch loss: 2.9181082248687744\n",
      "| epoch: 30 | batch: 17200/74328 | batch loss: 3.5263726711273193\n",
      "| epoch: 30 | batch: 17400/74328 | batch loss: 2.3472235202789307\n",
      "| epoch: 30 | batch: 17600/74328 | batch loss: 2.8968300819396973\n",
      "| epoch: 30 | batch: 17800/74328 | batch loss: 2.4752297401428223\n",
      "| epoch: 30 | batch: 18000/74328 | batch loss: 2.822418212890625\n",
      "| epoch: 30 | batch: 18200/74328 | batch loss: 2.6239585876464844\n",
      "| epoch: 30 | batch: 18400/74328 | batch loss: 2.595503568649292\n",
      "| epoch: 30 | batch: 18600/74328 | batch loss: 2.284332036972046\n",
      "| epoch: 30 | batch: 18800/74328 | batch loss: 2.5085432529449463\n",
      "| epoch: 30 | batch: 19000/74328 | batch loss: 3.8014841079711914\n",
      "| epoch: 30 | batch: 19200/74328 | batch loss: 2.273510456085205\n",
      "| epoch: 30 | batch: 19400/74328 | batch loss: 2.549065589904785\n",
      "| epoch: 30 | batch: 19600/74328 | batch loss: 2.9621262550354004\n",
      "| epoch: 30 | batch: 19800/74328 | batch loss: 2.0700926780700684\n",
      "| epoch: 30 | batch: 20000/74328 | batch loss: 4.362508296966553\n",
      "| epoch: 30 | batch: 20200/74328 | batch loss: 2.793888807296753\n",
      "| epoch: 30 | batch: 20400/74328 | batch loss: 1.4401606321334839\n",
      "| epoch: 30 | batch: 20600/74328 | batch loss: 1.8402107954025269\n",
      "| epoch: 30 | batch: 20800/74328 | batch loss: 2.7971549034118652\n",
      "| epoch: 30 | batch: 21000/74328 | batch loss: 2.6572721004486084\n",
      "| epoch: 30 | batch: 21200/74328 | batch loss: 3.3721022605895996\n",
      "| epoch: 30 | batch: 21400/74328 | batch loss: 2.1756629943847656\n",
      "| epoch: 30 | batch: 21600/74328 | batch loss: 2.558997631072998\n",
      "| epoch: 30 | batch: 21800/74328 | batch loss: 2.295731544494629\n",
      "| epoch: 30 | batch: 22000/74328 | batch loss: 3.142209529876709\n",
      "| epoch: 30 | batch: 22200/74328 | batch loss: 2.313098669052124\n",
      "| epoch: 30 | batch: 22400/74328 | batch loss: 3.2343642711639404\n",
      "| epoch: 30 | batch: 22600/74328 | batch loss: 3.9578800201416016\n",
      "| epoch: 30 | batch: 22800/74328 | batch loss: 3.4337522983551025\n",
      "| epoch: 30 | batch: 23000/74328 | batch loss: 2.281097173690796\n",
      "| epoch: 30 | batch: 23200/74328 | batch loss: 2.8628125190734863\n",
      "| epoch: 30 | batch: 23400/74328 | batch loss: 3.0110013484954834\n",
      "| epoch: 30 | batch: 23600/74328 | batch loss: 1.962734341621399\n",
      "| epoch: 30 | batch: 23800/74328 | batch loss: 2.6501476764678955\n",
      "| epoch: 30 | batch: 24000/74328 | batch loss: 3.5986106395721436\n",
      "| epoch: 30 | batch: 24200/74328 | batch loss: 1.8783434629440308\n",
      "| epoch: 30 | batch: 24400/74328 | batch loss: 2.5703234672546387\n",
      "| epoch: 30 | batch: 24600/74328 | batch loss: 1.9355295896530151\n",
      "| epoch: 30 | batch: 24800/74328 | batch loss: 3.8214111328125\n",
      "| epoch: 30 | batch: 25000/74328 | batch loss: 1.8847670555114746\n",
      "| epoch: 30 | batch: 25200/74328 | batch loss: 2.5998313426971436\n",
      "| epoch: 30 | batch: 25400/74328 | batch loss: 1.6237131357192993\n",
      "| epoch: 30 | batch: 25600/74328 | batch loss: 1.9311975240707397\n",
      "| epoch: 30 | batch: 25800/74328 | batch loss: 2.4060471057891846\n",
      "| epoch: 30 | batch: 26000/74328 | batch loss: 2.14855694770813\n",
      "| epoch: 30 | batch: 26200/74328 | batch loss: 2.339736223220825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 30 | batch: 26400/74328 | batch loss: 4.419778823852539\n",
      "| epoch: 30 | batch: 26600/74328 | batch loss: 2.473423480987549\n",
      "| epoch: 30 | batch: 26800/74328 | batch loss: 3.0484859943389893\n",
      "| epoch: 30 | batch: 27000/74328 | batch loss: 3.202897310256958\n",
      "| epoch: 30 | batch: 27200/74328 | batch loss: 2.5265324115753174\n",
      "| epoch: 30 | batch: 27400/74328 | batch loss: 2.839341878890991\n",
      "| epoch: 30 | batch: 27600/74328 | batch loss: 3.122657060623169\n",
      "| epoch: 30 | batch: 27800/74328 | batch loss: 3.7506020069122314\n",
      "| epoch: 30 | batch: 28000/74328 | batch loss: 1.9508042335510254\n",
      "| epoch: 30 | batch: 28200/74328 | batch loss: 1.88569974899292\n",
      "| epoch: 30 | batch: 28400/74328 | batch loss: 2.1049468517303467\n",
      "| epoch: 30 | batch: 28600/74328 | batch loss: 2.4604616165161133\n",
      "| epoch: 30 | batch: 28800/74328 | batch loss: 3.3811206817626953\n",
      "| epoch: 30 | batch: 29000/74328 | batch loss: 2.7187821865081787\n",
      "| epoch: 30 | batch: 29200/74328 | batch loss: 2.5065298080444336\n",
      "| epoch: 30 | batch: 29400/74328 | batch loss: 2.701812744140625\n",
      "| epoch: 30 | batch: 29600/74328 | batch loss: 2.3245625495910645\n",
      "| epoch: 30 | batch: 29800/74328 | batch loss: 2.237454891204834\n",
      "| epoch: 30 | batch: 30000/74328 | batch loss: 2.977696657180786\n",
      "| epoch: 30 | batch: 30200/74328 | batch loss: 2.721747636795044\n",
      "| epoch: 30 | batch: 30400/74328 | batch loss: 1.9568639993667603\n",
      "| epoch: 30 | batch: 30600/74328 | batch loss: 4.466604232788086\n",
      "| epoch: 30 | batch: 30800/74328 | batch loss: 2.134247303009033\n",
      "| epoch: 30 | batch: 31000/74328 | batch loss: 3.126145839691162\n",
      "| epoch: 30 | batch: 31200/74328 | batch loss: 2.4308359622955322\n",
      "| epoch: 30 | batch: 31400/74328 | batch loss: 2.2668681144714355\n",
      "| epoch: 30 | batch: 31600/74328 | batch loss: 2.297945499420166\n",
      "| epoch: 30 | batch: 31800/74328 | batch loss: 2.4699690341949463\n",
      "| epoch: 30 | batch: 32000/74328 | batch loss: 3.6803908348083496\n",
      "| epoch: 30 | batch: 32200/74328 | batch loss: 2.5333032608032227\n",
      "| epoch: 30 | batch: 32400/74328 | batch loss: 1.8770816326141357\n",
      "| epoch: 30 | batch: 32600/74328 | batch loss: 2.057267189025879\n",
      "| epoch: 30 | batch: 32800/74328 | batch loss: 2.5537545680999756\n",
      "| epoch: 30 | batch: 33000/74328 | batch loss: 1.7137309312820435\n",
      "| epoch: 30 | batch: 33200/74328 | batch loss: 3.3611130714416504\n",
      "| epoch: 30 | batch: 33400/74328 | batch loss: 3.0238211154937744\n",
      "| epoch: 30 | batch: 33600/74328 | batch loss: 3.3930578231811523\n",
      "| epoch: 30 | batch: 33800/74328 | batch loss: 2.068314552307129\n",
      "| epoch: 30 | batch: 34000/74328 | batch loss: 3.0155701637268066\n",
      "| epoch: 30 | batch: 34200/74328 | batch loss: 3.553084135055542\n",
      "| epoch: 30 | batch: 34400/74328 | batch loss: 1.79122793674469\n",
      "| epoch: 30 | batch: 34600/74328 | batch loss: 3.4008429050445557\n",
      "| epoch: 30 | batch: 34800/74328 | batch loss: 3.397953987121582\n",
      "| epoch: 30 | batch: 35000/74328 | batch loss: 2.61506724357605\n",
      "| epoch: 30 | batch: 35200/74328 | batch loss: 1.9398528337478638\n",
      "| epoch: 30 | batch: 35400/74328 | batch loss: 2.2793400287628174\n",
      "| epoch: 30 | batch: 35600/74328 | batch loss: 2.7947468757629395\n",
      "| epoch: 30 | batch: 35800/74328 | batch loss: 2.731760025024414\n",
      "| epoch: 30 | batch: 36000/74328 | batch loss: 2.8532485961914062\n",
      "| epoch: 30 | batch: 36200/74328 | batch loss: 2.538135528564453\n",
      "| epoch: 30 | batch: 36400/74328 | batch loss: 1.9075716733932495\n",
      "| epoch: 30 | batch: 36600/74328 | batch loss: 2.7186503410339355\n",
      "| epoch: 30 | batch: 36800/74328 | batch loss: 2.8728044033050537\n",
      "| epoch: 30 | batch: 37000/74328 | batch loss: 1.9653749465942383\n",
      "| epoch: 30 | batch: 37200/74328 | batch loss: 1.8059073686599731\n",
      "| epoch: 30 | batch: 37400/74328 | batch loss: 3.079725980758667\n",
      "| epoch: 30 | batch: 37600/74328 | batch loss: 2.6018970012664795\n",
      "| epoch: 30 | batch: 37800/74328 | batch loss: 2.6092379093170166\n",
      "| epoch: 30 | batch: 38000/74328 | batch loss: 2.1178691387176514\n",
      "| epoch: 30 | batch: 38200/74328 | batch loss: 3.272920608520508\n",
      "| epoch: 30 | batch: 38400/74328 | batch loss: 2.203977108001709\n",
      "| epoch: 30 | batch: 38600/74328 | batch loss: 2.9842703342437744\n",
      "| epoch: 30 | batch: 38800/74328 | batch loss: 3.059934616088867\n",
      "| epoch: 30 | batch: 39000/74328 | batch loss: 2.513568162918091\n",
      "| epoch: 30 | batch: 39200/74328 | batch loss: 2.601914644241333\n",
      "| epoch: 30 | batch: 39400/74328 | batch loss: 2.362532377243042\n",
      "| epoch: 30 | batch: 39600/74328 | batch loss: 3.0520973205566406\n",
      "| epoch: 30 | batch: 39800/74328 | batch loss: 1.8658779859542847\n",
      "| epoch: 30 | batch: 40000/74328 | batch loss: 3.6847643852233887\n",
      "| epoch: 30 | batch: 40200/74328 | batch loss: 3.7865636348724365\n",
      "| epoch: 30 | batch: 40400/74328 | batch loss: 1.7226258516311646\n",
      "| epoch: 30 | batch: 40600/74328 | batch loss: 2.1679720878601074\n",
      "| epoch: 30 | batch: 40800/74328 | batch loss: 5.772202014923096\n",
      "| epoch: 30 | batch: 41000/74328 | batch loss: 2.626142740249634\n",
      "| epoch: 30 | batch: 41200/74328 | batch loss: 2.0440948009490967\n",
      "| epoch: 30 | batch: 41400/74328 | batch loss: 2.312542676925659\n",
      "| epoch: 30 | batch: 41600/74328 | batch loss: 2.40128493309021\n",
      "| epoch: 30 | batch: 41800/74328 | batch loss: 4.480149269104004\n",
      "| epoch: 30 | batch: 42000/74328 | batch loss: 3.3516082763671875\n",
      "| epoch: 30 | batch: 42200/74328 | batch loss: 3.5676920413970947\n",
      "| epoch: 30 | batch: 42400/74328 | batch loss: 2.643221378326416\n",
      "| epoch: 30 | batch: 42600/74328 | batch loss: 2.25972056388855\n",
      "| epoch: 30 | batch: 42800/74328 | batch loss: 2.2776427268981934\n",
      "| epoch: 30 | batch: 43000/74328 | batch loss: 2.234248638153076\n",
      "| epoch: 30 | batch: 43200/74328 | batch loss: 2.407841444015503\n",
      "| epoch: 30 | batch: 43400/74328 | batch loss: 1.9287968873977661\n",
      "| epoch: 30 | batch: 43600/74328 | batch loss: 3.5204873085021973\n",
      "| epoch: 30 | batch: 43800/74328 | batch loss: 2.9954590797424316\n",
      "| epoch: 30 | batch: 44000/74328 | batch loss: 2.0513784885406494\n",
      "| epoch: 30 | batch: 44200/74328 | batch loss: 2.675870180130005\n",
      "| epoch: 30 | batch: 44400/74328 | batch loss: 2.631498336791992\n",
      "| epoch: 30 | batch: 44600/74328 | batch loss: 2.9150283336639404\n",
      "| epoch: 30 | batch: 44800/74328 | batch loss: 2.3101534843444824\n",
      "| epoch: 30 | batch: 45000/74328 | batch loss: 1.9514695405960083\n",
      "| epoch: 30 | batch: 45200/74328 | batch loss: 2.359328031539917\n",
      "| epoch: 30 | batch: 45400/74328 | batch loss: 1.9293508529663086\n",
      "| epoch: 30 | batch: 45600/74328 | batch loss: 1.9755765199661255\n",
      "| epoch: 30 | batch: 45800/74328 | batch loss: 3.2386460304260254\n",
      "| epoch: 30 | batch: 46000/74328 | batch loss: 1.7593317031860352\n",
      "| epoch: 30 | batch: 46200/74328 | batch loss: 3.1493442058563232\n",
      "| epoch: 30 | batch: 46400/74328 | batch loss: 3.1140198707580566\n",
      "| epoch: 30 | batch: 46600/74328 | batch loss: 2.4537553787231445\n",
      "| epoch: 30 | batch: 46800/74328 | batch loss: 2.4633991718292236\n",
      "| epoch: 30 | batch: 47000/74328 | batch loss: 2.0589404106140137\n",
      "| epoch: 30 | batch: 47200/74328 | batch loss: 1.530958890914917\n",
      "| epoch: 30 | batch: 47400/74328 | batch loss: 2.7912299633026123\n",
      "| epoch: 30 | batch: 47600/74328 | batch loss: 2.648545980453491\n",
      "| epoch: 30 | batch: 47800/74328 | batch loss: 2.7647087574005127\n",
      "| epoch: 30 | batch: 48000/74328 | batch loss: 3.0556299686431885\n",
      "| epoch: 30 | batch: 48200/74328 | batch loss: 2.5596768856048584\n",
      "| epoch: 30 | batch: 48400/74328 | batch loss: 2.8541722297668457\n",
      "| epoch: 30 | batch: 48600/74328 | batch loss: 3.5295348167419434\n",
      "| epoch: 30 | batch: 48800/74328 | batch loss: 3.9699418544769287\n",
      "| epoch: 30 | batch: 49000/74328 | batch loss: 2.8784565925598145\n",
      "| epoch: 30 | batch: 49200/74328 | batch loss: 2.2187740802764893\n",
      "| epoch: 30 | batch: 49400/74328 | batch loss: 4.760887622833252\n",
      "| epoch: 30 | batch: 49600/74328 | batch loss: 2.470468282699585\n",
      "| epoch: 30 | batch: 49800/74328 | batch loss: 3.4436745643615723\n",
      "| epoch: 30 | batch: 50000/74328 | batch loss: 3.0790817737579346\n",
      "| epoch: 30 | batch: 50200/74328 | batch loss: 3.4935476779937744\n",
      "| epoch: 30 | batch: 50400/74328 | batch loss: 3.781546115875244\n",
      "| epoch: 30 | batch: 50600/74328 | batch loss: 2.8373806476593018\n",
      "| epoch: 30 | batch: 50800/74328 | batch loss: 3.8772239685058594\n",
      "| epoch: 30 | batch: 51000/74328 | batch loss: 2.7265875339508057\n",
      "| epoch: 30 | batch: 51200/74328 | batch loss: 2.6681058406829834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 30 | batch: 51400/74328 | batch loss: 3.0220561027526855\n",
      "| epoch: 30 | batch: 51600/74328 | batch loss: 2.1246721744537354\n",
      "| epoch: 30 | batch: 51800/74328 | batch loss: 2.569762945175171\n",
      "| epoch: 30 | batch: 52000/74328 | batch loss: 3.3981142044067383\n",
      "| epoch: 30 | batch: 52200/74328 | batch loss: 2.06313419342041\n",
      "| epoch: 30 | batch: 52400/74328 | batch loss: 2.6149075031280518\n",
      "| epoch: 30 | batch: 52600/74328 | batch loss: 3.081749200820923\n",
      "| epoch: 30 | batch: 52800/74328 | batch loss: 4.1751203536987305\n",
      "| epoch: 30 | batch: 53000/74328 | batch loss: 2.1405935287475586\n",
      "| epoch: 30 | batch: 53200/74328 | batch loss: 4.416858196258545\n",
      "| epoch: 30 | batch: 53400/74328 | batch loss: 3.541029453277588\n",
      "| epoch: 30 | batch: 53600/74328 | batch loss: 2.3038017749786377\n",
      "| epoch: 30 | batch: 53800/74328 | batch loss: 2.3277804851531982\n",
      "| epoch: 30 | batch: 54000/74328 | batch loss: 1.4124348163604736\n",
      "| epoch: 30 | batch: 54200/74328 | batch loss: 3.3595423698425293\n",
      "| epoch: 30 | batch: 54400/74328 | batch loss: 2.8013837337493896\n",
      "| epoch: 30 | batch: 54600/74328 | batch loss: 2.447918653488159\n",
      "| epoch: 30 | batch: 54800/74328 | batch loss: 2.9948623180389404\n",
      "| epoch: 30 | batch: 55000/74328 | batch loss: 2.84877347946167\n",
      "| epoch: 30 | batch: 55200/74328 | batch loss: 2.454619884490967\n",
      "| epoch: 30 | batch: 55400/74328 | batch loss: 2.6240041255950928\n",
      "| epoch: 30 | batch: 55600/74328 | batch loss: 1.8166786432266235\n",
      "| epoch: 30 | batch: 55800/74328 | batch loss: 2.801983118057251\n",
      "| epoch: 30 | batch: 56000/74328 | batch loss: 2.706498861312866\n",
      "| epoch: 30 | batch: 56200/74328 | batch loss: 2.45995831489563\n",
      "| epoch: 30 | batch: 56400/74328 | batch loss: 2.6454408168792725\n",
      "| epoch: 30 | batch: 56600/74328 | batch loss: 2.5459344387054443\n",
      "| epoch: 30 | batch: 56800/74328 | batch loss: 3.1823623180389404\n",
      "| epoch: 30 | batch: 57000/74328 | batch loss: 2.740165948867798\n",
      "| epoch: 30 | batch: 57200/74328 | batch loss: 2.523543119430542\n",
      "| epoch: 30 | batch: 57400/74328 | batch loss: 3.030348539352417\n",
      "| epoch: 30 | batch: 57600/74328 | batch loss: 3.305551767349243\n",
      "| epoch: 30 | batch: 57800/74328 | batch loss: 2.7601678371429443\n",
      "| epoch: 30 | batch: 58000/74328 | batch loss: 2.053473711013794\n",
      "| epoch: 30 | batch: 58200/74328 | batch loss: 2.6924643516540527\n",
      "| epoch: 30 | batch: 58400/74328 | batch loss: 2.7568440437316895\n",
      "| epoch: 30 | batch: 58600/74328 | batch loss: 3.072467803955078\n",
      "| epoch: 30 | batch: 58800/74328 | batch loss: 2.566256284713745\n",
      "| epoch: 30 | batch: 59000/74328 | batch loss: 2.1118006706237793\n",
      "| epoch: 30 | batch: 59200/74328 | batch loss: 3.356685161590576\n",
      "| epoch: 30 | batch: 59400/74328 | batch loss: 2.2965245246887207\n",
      "| epoch: 30 | batch: 59600/74328 | batch loss: 2.7959516048431396\n",
      "| epoch: 30 | batch: 59800/74328 | batch loss: 1.7124043703079224\n",
      "| epoch: 30 | batch: 60000/74328 | batch loss: 2.248840808868408\n",
      "| epoch: 30 | batch: 60200/74328 | batch loss: 2.037466526031494\n",
      "| epoch: 30 | batch: 60400/74328 | batch loss: 1.9182802438735962\n",
      "| epoch: 30 | batch: 60600/74328 | batch loss: 3.6378846168518066\n",
      "| epoch: 30 | batch: 60800/74328 | batch loss: 2.4729866981506348\n",
      "| epoch: 30 | batch: 61000/74328 | batch loss: 3.491093397140503\n",
      "| epoch: 30 | batch: 61200/74328 | batch loss: 2.7969300746917725\n",
      "| epoch: 30 | batch: 61400/74328 | batch loss: 2.3032455444335938\n",
      "| epoch: 30 | batch: 61600/74328 | batch loss: 3.003763198852539\n",
      "| epoch: 30 | batch: 61800/74328 | batch loss: 2.97921085357666\n",
      "| epoch: 30 | batch: 62000/74328 | batch loss: 2.0386435985565186\n",
      "| epoch: 30 | batch: 62200/74328 | batch loss: 2.939335346221924\n",
      "| epoch: 30 | batch: 62400/74328 | batch loss: 2.9272332191467285\n",
      "| epoch: 30 | batch: 62600/74328 | batch loss: 2.794257164001465\n",
      "| epoch: 30 | batch: 62800/74328 | batch loss: 3.6688883304595947\n",
      "| epoch: 30 | batch: 63000/74328 | batch loss: 1.825775384902954\n",
      "| epoch: 30 | batch: 63200/74328 | batch loss: 3.116222381591797\n",
      "| epoch: 30 | batch: 63400/74328 | batch loss: 2.2782094478607178\n",
      "| epoch: 30 | batch: 63600/74328 | batch loss: 1.8834173679351807\n",
      "| epoch: 30 | batch: 63800/74328 | batch loss: 2.6313374042510986\n",
      "| epoch: 30 | batch: 64000/74328 | batch loss: 3.163156032562256\n",
      "| epoch: 30 | batch: 64200/74328 | batch loss: 2.1655449867248535\n",
      "| epoch: 30 | batch: 64400/74328 | batch loss: 2.311782121658325\n",
      "| epoch: 30 | batch: 64600/74328 | batch loss: 3.226292610168457\n",
      "| epoch: 30 | batch: 64800/74328 | batch loss: 3.549172878265381\n",
      "| epoch: 30 | batch: 65000/74328 | batch loss: 3.0642294883728027\n",
      "| epoch: 30 | batch: 65200/74328 | batch loss: 2.0230071544647217\n",
      "| epoch: 30 | batch: 65400/74328 | batch loss: 3.0288617610931396\n",
      "| epoch: 30 | batch: 65600/74328 | batch loss: 2.3635833263397217\n",
      "| epoch: 30 | batch: 65800/74328 | batch loss: 2.164562225341797\n",
      "| epoch: 30 | batch: 66000/74328 | batch loss: 2.918382406234741\n",
      "| epoch: 30 | batch: 66200/74328 | batch loss: 2.548509120941162\n",
      "| epoch: 30 | batch: 66400/74328 | batch loss: 3.444880962371826\n",
      "| epoch: 30 | batch: 66600/74328 | batch loss: 2.284900665283203\n",
      "| epoch: 30 | batch: 66800/74328 | batch loss: 2.0779781341552734\n",
      "| epoch: 30 | batch: 67000/74328 | batch loss: 3.3452253341674805\n",
      "| epoch: 30 | batch: 67200/74328 | batch loss: 1.864081859588623\n",
      "| epoch: 30 | batch: 67400/74328 | batch loss: 2.6734249591827393\n",
      "| epoch: 30 | batch: 67600/74328 | batch loss: 3.3225057125091553\n",
      "| epoch: 30 | batch: 67800/74328 | batch loss: 3.2089881896972656\n",
      "| epoch: 30 | batch: 68000/74328 | batch loss: 1.7706847190856934\n",
      "| epoch: 30 | batch: 68200/74328 | batch loss: 1.9837076663970947\n",
      "| epoch: 30 | batch: 68400/74328 | batch loss: 2.6411561965942383\n",
      "| epoch: 30 | batch: 68600/74328 | batch loss: 1.810071587562561\n",
      "| epoch: 30 | batch: 68800/74328 | batch loss: 1.6601839065551758\n",
      "| epoch: 30 | batch: 69000/74328 | batch loss: 3.077908754348755\n",
      "| epoch: 30 | batch: 69200/74328 | batch loss: 2.3839519023895264\n",
      "| epoch: 30 | batch: 69400/74328 | batch loss: 2.0202767848968506\n",
      "| epoch: 30 | batch: 69600/74328 | batch loss: 1.961197018623352\n",
      "| epoch: 30 | batch: 69800/74328 | batch loss: 2.285888195037842\n",
      "| epoch: 30 | batch: 70000/74328 | batch loss: 2.3312742710113525\n",
      "| epoch: 30 | batch: 70200/74328 | batch loss: 2.774303674697876\n",
      "| epoch: 30 | batch: 70400/74328 | batch loss: 3.839040517807007\n",
      "| epoch: 30 | batch: 70600/74328 | batch loss: 2.435002565383911\n",
      "| epoch: 30 | batch: 70800/74328 | batch loss: 2.9643452167510986\n",
      "| epoch: 30 | batch: 71000/74328 | batch loss: 2.4652187824249268\n",
      "| epoch: 30 | batch: 71200/74328 | batch loss: 2.3711373805999756\n",
      "| epoch: 30 | batch: 71400/74328 | batch loss: 2.9957399368286133\n",
      "| epoch: 30 | batch: 71600/74328 | batch loss: 2.5619072914123535\n",
      "| epoch: 30 | batch: 71800/74328 | batch loss: 1.940293550491333\n",
      "| epoch: 30 | batch: 72000/74328 | batch loss: 2.871367931365967\n",
      "| epoch: 30 | batch: 72200/74328 | batch loss: 3.676509141921997\n",
      "| epoch: 30 | batch: 72400/74328 | batch loss: 2.7093358039855957\n",
      "| epoch: 30 | batch: 72600/74328 | batch loss: 2.8581299781799316\n",
      "| epoch: 30 | batch: 72800/74328 | batch loss: 2.1811153888702393\n",
      "| epoch: 30 | batch: 73000/74328 | batch loss: 2.5736846923828125\n",
      "| epoch: 30 | batch: 73200/74328 | batch loss: 2.2515299320220947\n",
      "| epoch: 30 | batch: 73400/74328 | batch loss: 3.3951356410980225\n",
      "| epoch: 30 | batch: 73600/74328 | batch loss: 2.5761983394622803\n",
      "| epoch: 30 | batch: 73800/74328 | batch loss: 2.812925338745117\n",
      "| epoch: 30 | batch: 74000/74328 | batch loss: 2.613487720489502\n",
      "| epoch: 30 | batch: 74200/74328 | batch loss: 2.7650835514068604\n",
      "Train Keypoint Loss (avg): 2.6987\n",
      "\n",
      "---------------------- epoch 30 ------------------------\n",
      "Train Keypoint Loss: 2.6987, Val Keypoint Loss: 1.7207\n",
      "Completed in 718m 18s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a710a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d441847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p38",
   "language": "python",
   "name": "pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
